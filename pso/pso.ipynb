{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# python-pso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Swarm Optimizer (PSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a common implementation of the PSO, only with some extensions. Meaning the algorithm runs a set number of iterations or if the sd of the swarm is less than a predefined value (this is relatively arbitrary, in this case the square root of the product(self.n * self.dims), which seemed to be a reasonable idea).\n",
    "\n",
    "Every iteration consists mainly of two steps:\n",
    "\n",
    "- evaluation the field at a given place for every particle\n",
    "- and updating it's position according to their personal best solution and the best solution found in the swarm.\n",
    "\n",
    "Every particle has a position in space and a vector defining it's velocity. As indicated, if a solution is better, it's updated. In this implementation you can set the relative time, when to start taking into account the global best solution. This can be done to give personal best solutions a greater impact to not miss global minima. For more complex problems you should consider using H-PSO or even PH-HPSO. Some more things I changed about the original algorithm:\n",
    "\n",
    "- if the velocity is to low, speed up with a rand_speed_factor, correlating with the dimension of the problem.\n",
    "- don't exceed a maximum speed.\n",
    "- chaos for velocity\n",
    "- limit the search space to the given problem (-n,n)\n",
    "\n",
    "Some of them could be left out or extended or could be set in motion in a more elegant way. Some of the characteristics, e.g. the pair-wise deviation of the swarm I found in the paper or the script for swarm intelligence, others were found in an trial-and-error way. I just want to illustrate the main principles for myself and everybody interested.\n",
    "\n",
    "The visualisation objects have to be initialized in a main method and passed as arguments. For more information about the classes look at 'my_visualisation.py'\n",
    "\n",
    "\n",
    "![hoelder_table](pso-example.png)\n",
    "\n",
    "See below for some example code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies and import the class PSO:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from ..helper.background_function import *\n",
    "from ..vis.PSOVisualization import Particle_2DVis\n",
    "from common_pso import PSO\n",
    "from hpso import HPSO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of particles and the maximum number of iterations you want to run. The number of dimensions is set to 2 to show the illustration. I implemented some benchmark functions, which are evaluated and plotted."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_particles = 13\n",
    "num_runs = 100\n",
    "dims = 2\n",
    "func_name = 'hoelder_table'\n",
    "target_array = np.zeros((num_runs, num_particles, dims))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate and instance of the PSO class and set the name of the function, which is to be evaluated (done by a dict-lookup)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pso = PSO(num_particles=num_particles,\n",
    "          dims=dims,\n",
    "          n=n)\n",
    "pso.set_func_name(func_name)\n",
    "print(pso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the global update frame, i.e. the iterations, when the updates are made and run the optimizer."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pso.set_global_update_frame(start=0.2, end=0.9, num_runs=num_runs)\n",
    "pso.run_pso(target_array=target_array,\n",
    "            create_vis=create_vis,\n",
    "            show_error_vis=False,\n",
    "            show_2dvis=True, \n",
    "            num_runs=num_runs)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step PSO Tutorial\n",
    "\n",
    "Below you find a step by step tutorial of a simplified, yet more performant variant of the PSO algorithm. \n",
    "Asside from numpy no external dependency is needed. Make sure to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to evaluate a function, we need to define it before starting. I choose the Rastrigin function, which is a little more complex than x^2 but not too complicated. It can be replaced by any number of functions applicable to arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rastrigin(row: np.ndarray):\n",
    "    \"\"\"\n",
    "        apply rastrigin-function to matrix-row\n",
    "\n",
    "    :param row:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    f_arr = np.frompyfunc(lambda d: d ** 2 - 10 * np.cos(np.pi * d) + 10, 1, 1)\n",
    "    return np.sum(f_arr(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to set some constants. n depends on the function, the variable dims sets the dimensionality. \n",
    "Values c1 & c2 determined empirically (not by me) and are needed for the PSO algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5.2\n",
    "dims = 3\n",
    "func = rastrigin\n",
    "\n",
    "c1 = c2 = 1.494\n",
    "\n",
    "num_particles = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize the positions of the particles and their velocities. Together they represent our swarm and it's movement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.random.rand(num_particles, dims) * n +1\n",
    "x = 2 * n * np.random.rand(num_particles, dims) - n\n",
    "assert v.shape == x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to save the current best solutions of the particles as well as the global best solution. They will influence the positions or rather the velocities of the particles in our swarm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_solution = np.full((1, num_particles), sys.maxsize)\n",
    "best_point = np.zeros_like(x)\n",
    "best_global_point = np.zeros(dims)\n",
    "best_global_solution = sys.maxsize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on to the good stuff. Since we defined a very nice function above, which makes it possible to apply a function to a row, we can apply it to all rows/ particles at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx = np.apply_along_axis(func, axis=1, arr=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we determine the best solutions of all particles. If the current solution is not better than the best solution of a given particle, it is - of course - not changed at all. The index is also used to get the position of the best solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = fx < best_solution\n",
    "best_solution[idx] = fx[idx[0]]\n",
    "best_point[idx[0]] = x[idx[0], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similary, we want to update the global best solution and points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update global best solutions\n",
    "idx = np.argmin(best_solution)\n",
    "if best_solution[:, idx][0] < best_global_solution:\n",
    "    best_global_solution = best_solution[:,idx][0]\n",
    "    best_global_point = x[idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the best solutions, we can update the velocities of the particles respective to the positions of their best solutions:\n",
    "$$v = v + rand * c_1 * (best\\_point -x)$$\n",
    "The same is done with respect to position of the global best solution:\n",
    "$$v = v + rand * c_2 * (global\\_best\\_point -x)$$\n",
    "\n",
    "This will drag the particles into the area of the current best solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.ranf(2)\n",
    "v += r[0] * c1 * (best_point - x)\n",
    "v += c2 * r[1] * (best_global_point - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating the particles is done by adding the velocites to the positions. In this implementation we don't want the particles to escape the search space, i.e. $-n, n$, thus we use np.clip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "x += v\n",
    "x = np.clip(x, a_min=-n, a_max=n)\n",
    "print(f\"{best_global_solution}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = np.random.ranf(dims)\n",
    "# update highest velocity\n",
    "if np.sum(np.abs(particle.v)) < np.sum(np.abs(max_vel[,])):\n",
    "    max_vel = particle.v\n",
    "    \n",
    "# weird\n",
    "if np.sum(r1) < np.sum(particle.v):\n",
    "    v = r1 * v * max_vel ** rand_speed_factor\n",
    "    # select random dimension\n",
    "    ri = np.random.randint(dims)\n",
    "    # constant factor to keep the chaos realistic\n",
    "    x[ri] = r1[0] * ((2 * n) - n) * self.ws[i] * 0.45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Barely anthing happend? That's normal. The PSO algorithm is run iteratively, so lets add a for-loop an decreasing weights to decrease the influence of personal best solutions towards the end of the execution and wrap into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pso(num_particles, dims, iterations):\n",
    "\n",
    "\n",
    "    n = 5.2\n",
    "    func = rastrigin\n",
    "    c1 = c2 = 1.494    \n",
    "    \n",
    "    v = np.random.rand(num_particles, dims) * n +1\n",
    "    x = 2 * n * np.random.rand(num_particles, dims) - n\n",
    "    assert v.shape == x.shape\n",
    "\n",
    "    best_solution = np.full((1, num_particles), sys.maxsize)\n",
    "    best_point = np.zeros_like(x)\n",
    "    best_global_point = np.zeros(dims)\n",
    "    best_global_solution = sys.maxsize\n",
    "    ws = np.linspace(0.9, 0.4, iterations)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # apply function to all particles\n",
    "        fx = np.apply_along_axis(func, axis=1, arr=x)\n",
    "\n",
    "        # update best solutions of points\n",
    "        idx = fx < best_solution\n",
    "        best_solution[idx] = fx[idx[0]]\n",
    "        best_point[idx[0]] = x[idx[0], :]\n",
    "\n",
    "        # update global best solutions\n",
    "        idx = np.argmin(best_solution)\n",
    "        if best_solution[:,idx][0] < best_global_solution:\n",
    "            best_global_solution = best_solution[:,idx][0]\n",
    "            best_global_point = x[idx, :]\n",
    "\n",
    "        # update velocity after formula given constants c1 & c2, some randomness,\n",
    "        # and personal/ global best solutions.\n",
    "        r = np.random.ranf(2)\n",
    "        v = ws[i]*v + r[0] * c1 * (best_point - x)\n",
    "        # you can start the global updating later, if you want.\n",
    "        v += c2 * r[1] * (best_global_point - x)\n",
    "\n",
    "        # update position\n",
    "        x += v\n",
    "\n",
    "        # keep all particles in range [-n, n]\n",
    "        x = np.clip(x, a_min=-n, a_max=n)\n",
    "\n",
    "        print(f\"{best_global_solution}\")\n",
    "\n",
    "num_particles = 30\n",
    "dims = 10\n",
    "iterations = 100\n",
    "\n",
    "#run_pso(num_particles, dims, iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifications\n",
    "The Algorithm can be adjusted by adding even more chaos for both position and velocity. Let's first look at position, since it's easier to grasp. We first select a random dimension (you could also select multiple dimensions) and some particles, which we want to update. For this I chose 10% of the particles, which is arbitrary. After, the points are reset with a constant factor 0.45."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5.\n",
    "r_dim = np.random.randint(dims)\n",
    "r_particles = np.random.choice(range(num_particles), num_particles//10)\n",
    "x[r_particles, r_dim] = np.random.ranf() * ((2 * n) - n) * 0.45"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets add some randomness to the velocities of the particles. We will need to keep track of the highest velocity and set a minimum velocity. I also added some code to add values once the particles move too slow. Notice that the particles a completely new velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_speed = 0.1\n",
    "# don't fall asleep.\n",
    "max_velocity = np.zeros(dims)\n",
    "p_vel_abs = np.sum(np.abs(v), axis=1)\n",
    "# update all particles, which have a speed which is below our threshold\n",
    "v[p_vel_abs < lowest_speed, :] += np.random.ranf(dims)\n",
    "\n",
    "# largest absolute velocity sum larger than max_vel\n",
    "max_vel_sum = np.sum(np.abs(max_velocity))\n",
    "# check if any particle is moving faster \n",
    "if np.any(max_vel_sum > p_vel_abs):\n",
    "    # get index of particle with highest velocity\n",
    "    _idx = np.argmax(np.sum((v[ max_vel_sum < p_vel_abs , :]), axis=1))\n",
    "    # update max_velocity\n",
    "    max_velocity = v[_idx, :]\n",
    "# choose a random particle and update\n",
    "r_particles = np.random.choice(range(num_particles), num_particles // 10)\n",
    "# new velocities for selected particles\n",
    "vals = np.random.ranf((len(r_particles), dims)) * max_velocity\n",
    "v[r_particles, :] = vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code could be inserted directly into the main function. I chose to put into an inner function which let's me keep the code structure without moving everything into a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Particle Swarm Optimizer (HPSO)\n",
    "\n",
    "This is a basic implementation of the H-PSO-Algorithm. It's derived from PSO and has a tree-object. I figure this can be done way more elegantly, but then again, it's just for illustration. Most of the main-functions are called in recursive fashion. Since the tree-height shouldn't exceed 2 or max 3 levels, this can be implemented otherwise (not recursive).\n",
    "\n",
    "The particles are hold in a tree-structure: the particle with the global best solution should be in the root, local best solutions are kept in the lower levels and leafs (although they are just 'normal' particles). Every parent has a direct impact on it's child. First every particle checks it's current solution and updates it's personal best if it's better. Then the particles swapp their places. This is done in the following way: if the solution of a child is better (i.e. lower) than it's parent, they change places. This is done top-bottom.\n",
    "\n",
    "After, the particles change their position and speed according to their local and personal best solution. The weight changes with the level of the particle, ^HPSO and vHPSO are possible: vHPSO meaning the weight decreases towards the root. Unlike the PSO, I only added one addition, namely to stop if the diversity of the swarm is below a tolerance.\n",
    "\n",
    "So far there is no implementation of PH-PSO. A first try can be found in update_hpso. The visualisation is done like in the PSO class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code execution works almost exactly as seen above. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "func_name = 'hoelder_table'\n",
    "n = 10  # ausweitung des feldes\n",
    "num_children = 3\n",
    "height = 2 # please don't change\n",
    "\n",
    "num_particles = num_children ** height + num_children + 1\n",
    "\n",
    "target_array = np.zeros((num_runs, num_particles, dims))\n",
    "\n",
    "    \n",
    "hpso = HPSO(num_particles=num_particles,\n",
    "            dims=dims,\n",
    "            n=n, num_children=num_children, height=height)\n",
    "hpso.set_func_name(func_name=func_name)\n",
    "print(hpso)\n",
    "\n",
    "hpso.run_hpso(target_array, \n",
    "              num_runs=num_runs,\n",
    "              show_vis=True,\n",
    "              show_error_vis=False)\n",
    "\n",
    "hpso.get_hpso_best_solutions(hpso.tree.root)\n",
    "\n",
    "\n",
    "if dims==2:\n",
    "    vis2d = Particle_2DVis(n=n, num_runs=num_runs)\n",
    "    values, t_m = generate_2d_background(func_name, n)\n",
    "    vis2d.set_data(target_array, values, t_m)\n",
    "    vis2d.plot_contours()\n",
    "    vis2d.set_point_size(3.5)\n",
    "    vis2d.animate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant literature:\n",
    "\n",
    "James Kennedy and Russell Eberhart. Particle swarm optimization. In IEEE International Conference on Neural Networks (ICNN’95), volume 4, pages 1942–1947, Perth, Western Australia, November-December 1995. IEEE.\n",
    "\n",
    "Trelea. The particle swarm optimization algorithm: Convergence analysis and parameter selection. IPL: Information Processing Letters, 85, 2003.\n",
    "\n",
    "J. Kennedy and R. Mendes. Population structure and particle swarm performance. In David B. Fogel, Mohamed A. El-Sharkawi, Xin Yao, Garry Greenwood, Hitoshi Iba, Paul Marrow, and Mark Shackleton, ed- itors, Proceedings of the 2002 Congress on Evolutionary Computation CEC2002, pages 1671–1676. IEEE Press, 2002. ISBN 0-7803-7278-6.\n",
    "\n",
    "P. N. Suganthan. Particle swarm optimizer with neighborhood opera- tor. In 1999 Congress on Evolutionary Computation, pages 1958–1962, Piscataway, NJ, 1999. IEEE Service Center. Trelea."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
